import sqlite3
import os
import signal
import sys
import threading
import time
from contextlib import contextmanager

# Fun√ß√£o para inicializar o banco e criar tabela se n√£o existir
def init_db():
    try:
        db_path = os.path.join(os.path.dirname(__file__), 'historico_base.db')
        conn = sqlite3.connect(db_path, timeout=10)
        cursor = conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS historico (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                usuario TEXT,
                prompt TEXT,
                resposta TEXT,
                data DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        # Criar tabela de configura√ß√µes
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS configuracoes (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                api_key TEXT UNIQUE,
                modelo TEXT DEFAULT 'gpt-5',
                max_tokens INTEGER DEFAULT 8000,
                chunk_size INTEGER DEFAULT 8000,
                data_atualizacao DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        conn.commit()
        conn.close()
        print("‚úÖ Banco de dados inicializado com sucesso")
    except Exception as e:
        print(f"‚ùå Erro ao inicializar banco: {e}")

# Pool de conex√µes para SQLite
@contextmanager
def get_db_connection():
    conn = None
    try:
        db_path = os.path.join(os.path.dirname(__file__), 'historico_base.db')
        conn = sqlite3.connect(db_path, timeout=10)
        yield conn
    except Exception as e:
        if conn:
            conn.rollback()
        print(f"‚ùå Erro na conex√£o com banco: {e}")
        raise
    finally:
        if conn:
            conn.close()

# Inicializar banco ao iniciar app
init_db()

from flask import Flask, request, jsonify, send_from_directory, send_file
from flask_cors import CORS
import os
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()
app = Flask(__name__)
CORS(app)

# Configura√ß√µes de timeout
REQUEST_TIMEOUT = 120  # 2 minutos para requisi√ß√µes OpenAI
OPENAI_TIMEOUT = 90    # 1.5 minutos para OpenAI especificamente

# Middleware de monitoramento
@app.before_request
def before_request():
    request.start_time = time.time()

@app.after_request
def after_request(response):
    duration = time.time() - request.start_time
    if duration > 5:  # Log apenas requisi√ß√µes longas
        print(f"‚ö†Ô∏è Requisi√ß√£o lenta: {request.endpoint} - {duration:.2f}s")
    return response

# Middleware para limpeza de mem√≥ria
import gc

@app.teardown_appcontext  
def cleanup(exception):
    gc.collect()  # For√ßar garbage collection

# Tratamento de sinais para graceful shutdown
def signal_handler(signum, frame):
    print(f"\nüõë Recebido sinal {signum}. Finalizando aplica√ß√£o...")
    sys.exit(0)

signal.signal(signal.SIGTERM, signal_handler)
signal.signal(signal.SIGINT, signal_handler)

# Rota otimizada para servir arquivos est√°ticos 
@app.route('/static/<path:filename>')
def static_files(filename):
    try:
        static_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'analise-bid-ia-tools')
        return send_from_directory(static_path, filename, as_attachment=False, cache_timeout=3600)
    except Exception as e:
        print(f"‚ùå Erro ao servir arquivo est√°tico {filename}: {e}")
        return jsonify({'error': 'Arquivo n√£o encontrado'}), 404

# Inicializar o cliente OpenAI com configura√ß√µes otimizadas
client = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY"),
    timeout=OPENAI_TIMEOUT,
    max_retries=2
)

# Fun√ß√£o para processar requisi√ß√£o com timeout
def process_openai_request(messages, model, max_tokens):
    """Processa requisi√ß√£o OpenAI com controle de timeout"""
    try:
        print(f"ÔøΩ DEBUG: Preparando requisi√ß√£o para {model}...")
        print(f"   Max Tokens: {max_tokens}")
        print(f"   Messages count: {len(messages)}")
        
        # ‚ö†Ô∏è GPT-5 usa Responses API, n√£o Chat Completions!
        if model.startswith('gpt-5'):
            print("ÔøΩ Usando Responses API para GPT-5...")
            
            # Combinar mensagens para input √∫nico (Responses API requer input, n√£o messages)
            user_message = ""
            for msg in messages:
                if msg.get("role") == "user":
                    user_message = msg.get("content", "")
                    break
            
            response = client.responses.create(
                model=model,
                input=user_message,
                max_output_tokens=max_tokens,
                reasoning={"effort": "low"},  # Baixo esfor√ßo para velocidade
                text={"verbosity": "high"}  # Alta verbosidade para an√°lise completa
            )
            print(f"‚úÖ Resposta GPT-5 recebida | Output tokens: {max_tokens}")
            
            # Converter resposta para formato compat√≠vel com Chat Completions
            class CompatResponse:
                class Choice:
                    class Message:
                        def __init__(self, content):
                            self.content = content
                    def __init__(self, content):
                        self.message = self.Message(content)
                        self.finish_reason = "stop"
                
                def __init__(self, content):
                    self.choices = [self.Choice(content)]
            
            return CompatResponse(response.output_text), None
        
        else:
            # Chat Completions API para outros modelos (GPT-4, etc)
            print(f"üîÑ Usando Chat Completions API para {model}...")
            temperature = 0.7
            
            try:
                response = client.chat.completions.create(
                    model=model,
                    messages=messages,
                    max_completion_tokens=max_tokens,
                    temperature=temperature,
                    timeout=OPENAI_TIMEOUT
                )
                print(f"‚úÖ Usando max_completion_tokens: {max_tokens} | temperature: {temperature}")
                return response, None
            except TypeError:
                # Fallback para vers√£o antiga do SDK
                response = client.chat.completions.create(
                    model=model,
                    messages=messages,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    timeout=OPENAI_TIMEOUT
                )
                print(f"‚úÖ Usando max_tokens (compatibilidade): {max_tokens}")
                return response, None
                
    except Exception as e:
        print(f"‚ùå ERRO CR√çTICO em process_openai_request: {type(e).__name__}: {str(e)}")
        import traceback
        traceback.print_exc()
        return None, str(e)

# Fun√ß√£o ass√≠ncrona para salvar hist√≥rico
def save_to_history_async(usuario, prompt, resposta):
    """Salva hist√≥rico de forma ass√≠ncrona para n√£o bloquear resposta"""
    def save_task():
        try:
            with get_db_connection() as conn:
                cursor = conn.cursor()
                cursor.execute(
                    'INSERT INTO historico (usuario, prompt, resposta) VALUES (?, ?, ?)',
                    (usuario, str(prompt), resposta)
                )
                conn.commit()
                print("‚úÖ Hist√≥rico salvo com sucesso")
        except Exception as e:
            print(f"‚ùå Erro ao salvar hist√≥rico: {e}")
    
    # Executar em thread separada
    thread = threading.Thread(target=save_task)
    thread.daemon = True
    thread.start()

@app.route('/api/chat', methods=['POST'])
def chat():
    start_time = time.time()
    
    try:
        data = request.json
        messages = data.get('messages', [])
        model = data.get('model', 'gpt-4')
        max_tokens = min(data.get('max_tokens', 2000), 4000)  # Limitar tokens
        
        print("üöÄ === NOVA REQUISI√á√ÉO DE AN√ÅLISE ===")
        print(f"üìß Modelo: {model}")
        print(f"üî¢ Max Tokens: {max_tokens}")
        print(f"üìù Total de mensagens: {len(messages)}")
        print("=" * 50)

        # Valida√ß√£o b√°sica
        if not messages:
            return jsonify({'error': 'Nenhuma mensagem fornecida'}), 400
        
        if len(str(messages)) > 50000:  # Limitar tamanho do prompt
            return jsonify({'error': 'Prompt muito longo. Reduza o tamanho do texto.'}), 400

        # Processar requisi√ß√£o OpenAI
        response, error = process_openai_request(messages, model, max_tokens)
        
        if error:
            print(f"‚ùå ERRO na API OpenAI: {error}")
            return jsonify({'error': f'Erro na API OpenAI: {error}'}), 500
        
        if not response:
            print("‚ùå Response √© None!")
            return jsonify({'error': 'Resposta nula da OpenAI'}), 500

        if not response.choices:
            print("‚ùå Response.choices vazio!")
            return jsonify({'error': 'Resposta vazia da OpenAI (choices vazio)'}), 500

        content = response.choices[0].message.content
        
        if not content:
            print("‚ö†Ô∏è WARNING: Content √© None ou vazio!")
            print(f"   Finish reason: {response.choices[0].finish_reason}")
            content = "(Resposta vazia recebida da OpenAI)"
        
        processing_time = time.time() - start_time
        
        print("‚úÖ Resposta da OpenAI recebida com sucesso!")
        print(f"üìÑ Tamanho da resposta: {len(content) if content else 0} caracteres")
        print(f"‚è±Ô∏è Tempo de processamento: {processing_time:.2f}s")
        print("=" * 50)

        # Salvar hist√≥rico de forma ass√≠ncrona
        save_to_history_async(
            data.get('usuario', 'anonimo'),
            messages,
            content
        )

        return jsonify({
            'choices': [{
                'message': {
                    'content': content
                }
            }],
            'processing_time': round(processing_time, 2)
        })
        
    except Exception as e:
        processing_time = time.time() - start_time
        error_msg = f"Erro interno do servidor: {str(e)}"
        print(f"‚ùå ERRO GERAL: {error_msg}")
        print(f"‚è±Ô∏è Tempo at√© erro: {processing_time:.2f}s")
        print("=" * 50)
        return jsonify({'error': error_msg}), 500

@app.route('/api/health', methods=['GET'])
def health():
    """Endpoint de sa√∫de com informa√ß√µes detalhadas"""
    try:
        # Testar conex√£o com banco
        with get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute('SELECT COUNT(*) FROM historico')
            total_records = cursor.fetchone()[0]
        
        return jsonify({
            "status": "ok",
            "openai_configured": bool(os.getenv("OPENAI_API_KEY")),
            "database_working": True,
            "total_records": total_records,
            "timeout_config": {
                "request_timeout": REQUEST_TIMEOUT,
                "openai_timeout": OPENAI_TIMEOUT
            }
        })
    except Exception as e:
        return jsonify({
            "status": "error",
            "openai_configured": bool(os.getenv("OPENAI_API_KEY")),
            "database_working": False,
            "error": str(e)
        }), 500

@app.route('/api/historico', methods=['GET'])
def get_historico():
    """Endpoint otimizado para buscar hist√≥rico"""
    try:
        limit = request.args.get('limit', 50, type=int)
        offset = request.args.get('offset', 0, type=int)
        
        # Limitar resultados para evitar sobrecarga
        limit = min(limit, 100)
        
        with get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute(
                'SELECT id, usuario, prompt, resposta, data FROM historico ORDER BY data DESC LIMIT ? OFFSET ?',
                (limit, offset)
            )
            rows = cursor.fetchall()
            
            # Contar total de registros
            cursor.execute('SELECT COUNT(*) FROM historico')
            total = cursor.fetchone()[0]
        
        historico = [
            {
                'id': row[0],
                'usuario': row[1],
                'prompt': row[2][:500] + '...' if len(row[2]) > 500 else row[2],  # Truncar prompt longo
                'resposta': row[3][:1000] + '...' if len(row[3]) > 1000 else row[3],  # Truncar resposta longa
                'data': row[4]
            }
            for row in rows
        ]
        
        return jsonify({
            'historico': historico,
            'total': total,
            'limit': limit,
            'offset': offset
        })
    except Exception as e:
        print(f"‚ùå Erro ao buscar hist√≥rico: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/settings', methods=['GET'])
def get_settings():
    """Endpoint para recuperar configura√ß√µes do usu√°rio"""
    try:
        # Tentar obter API Key do header
        api_key = request.headers.get('X-API-Key') or request.args.get('api_key', 'default')
        
        with get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute(
                'SELECT modelo, max_tokens, chunk_size FROM configuracoes WHERE api_key = ?',
                (api_key,)
            )
            row = cursor.fetchone()
        
        if row:
            return jsonify({
                'modelo': row[0],
                'max_tokens': row[1],
                'chunk_size': row[2],
                'cached': False
            })
        else:
            # Retornar valores padr√£o se n√£o encontrado
            return jsonify({
                'modelo': 'gpt-5',
                'max_tokens': 8000,
                'chunk_size': 8000,
                'cached': True
            })
    except Exception as e:
        print(f"‚ùå Erro ao buscar configura√ß√µes: {e}")
        return jsonify({
            'modelo': 'gpt-5',
            'max_tokens': 8000,
            'chunk_size': 8000,
            'error': str(e),
            'cached': True
        }), 200  # Retornar 200 mesmo com erro para fallback

@app.route('/api/settings', methods=['POST'])
def save_settings():
    """Endpoint para salvar configura√ß√µes do usu√°rio"""
    try:
        data = request.json
        api_key = data.get('api_key', 'default')
        modelo = data.get('modelo', 'gpt-5')
        max_tokens = data.get('max_tokens', 8000)
        chunk_size = data.get('chunk_size', 8000)
        
        # Valida√ß√µes b√°sicas
        if max_tokens < 100 or max_tokens > 128000:
            return jsonify({'error': 'max_tokens deve estar entre 100 e 128000'}), 400
        
        if chunk_size < 100 or chunk_size > 128000:
            return jsonify({'error': 'chunk_size deve estar entre 100 e 128000'}), 400
        
        with get_db_connection() as conn:
            cursor = conn.cursor()
            # Tentar atualizar, sen√£o inserir (UPSERT)
            cursor.execute('''
                INSERT INTO configuracoes (api_key, modelo, max_tokens, chunk_size)
                VALUES (?, ?, ?, ?)
                ON CONFLICT(api_key) 
                DO UPDATE SET 
                    modelo = excluded.modelo,
                    max_tokens = excluded.max_tokens,
                    chunk_size = excluded.chunk_size,
                    data_atualizacao = CURRENT_TIMESTAMP
            ''', (api_key, modelo, max_tokens, chunk_size))
            conn.commit()
        
        print(f"‚úÖ Configura√ß√µes salvas para API Key: {api_key[:10]}...")
        return jsonify({
            'success': True,
            'message': 'Configura√ß√µes salvas com sucesso',
            'modelo': modelo,
            'max_tokens': max_tokens,
            'chunk_size': chunk_size
        })
    except Exception as e:
        print(f"‚ùå Erro ao salvar configura√ß√µes: {e}")
        return jsonify({'error': str(e)}), 500

# Rota otimizada para servir arquivos da pasta App-IA
@app.route('/App-IA/<path:filename>')
def serve_app_ia_files(filename):
    try:
        app_ia_path = os.path.join(os.path.dirname(os.path.dirname(__file__)))
        return send_from_directory(app_ia_path, filename, as_attachment=False, cache_timeout=3600)
    except Exception as e:
        print(f"‚ùå Erro ao servir arquivo App-IA {filename}: {e}")
        return jsonify({'error': 'Arquivo n√£o encontrado'}), 404

# Rota otimizada para a p√°gina principal
@app.route('/')
def index():
    try:
        app_ia_path = os.path.dirname(os.path.dirname(__file__))
        return send_file(os.path.join(app_ia_path, 'index.html'))
    except Exception as e:
        print(f"‚ùå Erro ao servir p√°gina principal: {e}")
        return jsonify({'error': 'P√°gina n√£o encontrada'}), 404

if __name__ == '__main__':
    print("=" * 70)
    print("üöÄ TOOLS ENGENHARIA - DOCUMENT AI ANALYZER BACKEND")
    print("=" * 70)
    print("üåê Servidor Flask iniciado em: http://localhost:5000")
    print("ü§ñ OpenAI API: Configurada e pronta")
    print("üìä Endpoints dispon√≠veis:")
    print("   ‚Ä¢ POST /api/chat - An√°lise de documentos")
    print("   ‚Ä¢ GET  /api/health - Status do sistema")
    print("   ‚Ä¢ GET  /api/historico - Hist√≥rico de an√°lises")
    print("   ‚Ä¢ GET  / - Interface principal")
    print("=" * 70)
    print("üí° Logs da aplica√ß√£o aparecer√£o abaixo:")
    print("=" * 70)
    
    # Para desenvolvimento local
    app.run(debug=False, port=5000, threaded=True)

# Para produ√ß√£o (Render/Vercel)
app = app