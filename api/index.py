import sqlite3
import os
import signal
import sys
import threading
import time
from contextlib import contextmanager
from datetime import datetime

# üîß Fun√ß√£o para for√ßar logs aparecerem em qualquer lugar
def log_debug(msg):
    """Force log to appear in Render/console AND file"""
    timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
    log_msg = f"[{timestamp}] {msg}"
    
    # 1. Console stdout
    print(log_msg, flush=True)
    sys.stdout.flush()
    
    # 2. Console stderr
    sys.stderr.write(f"{log_msg}\n")
    sys.stderr.flush()
    
    # 3. File (persists data)
    try:
        log_file = os.path.join(os.path.dirname(__file__), 'app_debug.log')
        with open(log_file, 'a', encoding='utf-8') as f:
            f.write(f"{log_msg}\n")
            f.flush()
    except:
        pass

# Fun√ß√£o para inicializar o banco e criar tabela se n√£o existir
def init_db():
    try:
        db_path = os.path.join(os.path.dirname(__file__), 'historico_base.db')
        conn = sqlite3.connect(db_path, timeout=10)
        cursor = conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS historico (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                usuario TEXT,
                prompt TEXT,
                resposta TEXT,
                data DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        # Criar tabela de configura√ß√µes
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS configuracoes (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                api_key TEXT UNIQUE,
                modelo TEXT DEFAULT 'gpt-5',
                max_tokens INTEGER DEFAULT 8000,
                chunk_size INTEGER DEFAULT 8000,
                data_atualizacao DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        conn.commit()
        conn.close()
        print("‚úÖ Banco de dados inicializado com sucesso")
    except Exception as e:
        print(f"‚ùå Erro ao inicializar banco: {e}")

# Pool de conex√µes para SQLite
@contextmanager
def get_db_connection():
    conn = None
    try:
        db_path = os.path.join(os.path.dirname(__file__), 'historico_base.db')
        conn = sqlite3.connect(db_path, timeout=10)
        yield conn
    except Exception as e:
        if conn:
            conn.rollback()
        print(f"‚ùå Erro na conex√£o com banco: {e}")
        raise
    finally:
        if conn:
            conn.close()

# Inicializar banco ao iniciar app
init_db()

from flask import Flask, request, jsonify, send_from_directory, send_file
from flask_cors import CORS
import os
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()
app = Flask(__name__)

# ‚úÖ CONFIGURAR CORS EXPLICITAMENTE
CORS(app, 
     origins=["https://analise-bid-ia-tools.vercel.app", "http://localhost:3000", "http://localhost:5000"],
     supports_credentials=True,
     allow_headers=["Content-Type", "Authorization"],
     methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"])

# Configura√ß√µes de timeout
REQUEST_TIMEOUT = 120  # 2 minutos para requisi√ß√µes OpenAI
OPENAI_TIMEOUT = 90    # 1.5 minutos para OpenAI especificamente

# Middleware de monitoramento
@app.before_request
def before_request():
    request.start_time = time.time()

@app.after_request
def after_request(response):
    duration = time.time() - request.start_time
    if duration > 5:  # Log apenas requisi√ß√µes longas
        print(f"‚ö†Ô∏è Requisi√ß√£o lenta: {request.endpoint} - {duration:.2f}s")
    return response

# Middleware para limpeza de mem√≥ria
import gc

@app.teardown_appcontext  
def cleanup(exception):
    gc.collect()  # For√ßar garbage collection

# Tratamento de sinais para graceful shutdown
def signal_handler(signum, frame):
    print(f"\nüõë Recebido sinal {signum}. Finalizando aplica√ß√£o...")
    sys.exit(0)

signal.signal(signal.SIGTERM, signal_handler)
signal.signal(signal.SIGINT, signal_handler)

# Rota otimizada para servir arquivos est√°ticos 
@app.route('/static/<path:filename>')
def static_files(filename):
    try:
        static_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'analise-bid-ia-tools')
        return send_from_directory(static_path, filename, as_attachment=False, cache_timeout=3600)
    except Exception as e:
        print(f"‚ùå Erro ao servir arquivo est√°tico {filename}: {e}")
        return jsonify({'error': 'Arquivo n√£o encontrado'}), 404

# Inicializar o cliente OpenAI com configura√ß√µes otimizadas
client = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY"),
    timeout=OPENAI_TIMEOUT,
    max_retries=2
)

# Fun√ß√£o para processar requisi√ß√£o com timeout
def process_openai_request(messages, model, max_tokens, retry_attempt=1):
    """Processa requisi√ß√£o OpenAI com controle de timeout"""
    try:
        log_debug(f"\nü§ñ === CHAMANDO OPENAI (Tentativa {retry_attempt}) ===")
        log_debug(f"   Model: {model}")
        log_debug(f"   Max Tokens: {max_tokens}")
        log_debug(f"   Temperature: 1 (GPT-5 obrigat√≥rio)")
        log_debug(f"   N√∫mero de mensagens: {len(messages)}")
        for idx, msg in enumerate(messages):
            msg_content = msg.get('content', '')
            msg_role = msg.get('role', 'unknown')
            content_preview = msg_content[:50] + "..." if len(msg_content) > 50 else msg_content
            log_debug(f"   ‚Ä¢ Mensagem {idx+1} ({msg_role}): {len(msg_content)} chars - {repr(content_preview)}")
        
        # Valida√ß√£o: mensagens n√£o podem estar vazias
        for msg in messages:
            if not msg.get('content') or not msg.get('content').strip():
                log_debug(f"‚ùå ERRO: Mensagem de role '{msg.get('role')}' est√° vazia!")
                raise ValueError(f"Mensagem de role '{msg.get('role')}' est√° vazia")
        
        # GPT-5 usa max_completion_tokens em vez de max_tokens
        # GPT-5 requer temperature=1 (n√£o suporta outros valores)
        log_debug("   ‚è≥ Aguardando resposta da OpenAI...")
        
        # Adicionar timeout mais curto para testar
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            max_completion_tokens=max_tokens,  # Corrigido para GPT-5
            temperature=1,  # GPT-5 s√≥ aceita valor padr√£o (1)
            timeout=OPENAI_TIMEOUT
        )
        log_debug("   ‚úÖ Resposta recebida da OpenAI")
        
        # üîç LOG DETALHADO DA RESPOSTA
        log_debug(f"\nüîé === ANALISANDO RESPOSTA DO OPENAI ===")
        log_debug(f"   Tipo do response: {type(response).__name__}")
        log_debug(f"   Has .choices: {hasattr(response, 'choices')}")
        
        if not hasattr(response, 'choices'):
            log_debug("   ‚ùå Response n√£o tem atributo 'choices'!")
            return response, None
            
        if not response.choices:
            log_debug("   ‚ùå response.choices est√° vazio!")
            return response, None
        
        choice = response.choices[0]
        log_debug(f"   N√∫mero de choices: {len(response.choices)}")
        log_debug(f"   Choice[0] type: {type(choice).__name__}")
        log_debug(f"   Has .message: {hasattr(choice, 'message')}")
        
        if not hasattr(choice, 'message'):
            log_debug("   ‚ùå Choice n√£o tem atributo 'message'!")
            return response, None
        
        msg = choice.message
        log_debug(f"   Message type: {type(msg).__name__}")
        log_debug(f"   Has .content: {hasattr(msg, 'content')}")
        
        if not hasattr(msg, 'content'):
            log_debug("   ‚ùå Message n√£o tem atributo 'content'!")
            return response, None
        
        content = msg.content
        log_debug(f"   Content type: {type(content).__name__}")
        log_debug(f"   Content value: {repr(content) if content else 'NULO/VAZIO'}")
        log_debug(f"   Content length: {len(content) if content else 0}")
        log_debug(f"   Is None: {content is None}")
        log_debug(f"   Is empty string: {content == ''}")
        log_debug(f"   Is whitespace only: {content.isspace() if isinstance(content, str) else 'N/A'}")
        log_debug("üîé === FIM DA AN√ÅLISE ===\n")
        
        return response, None
    except Exception as e:
        print(f"\n‚ùå ERRO ao chamar OpenAI:")
        print(f"   Tipo do erro: {type(e).__name__}")
        print(f"   Mensagem: {str(e)}")
        import traceback
        print(f"   Traceback: {traceback.format_exc()}")
        return None, str(e)

# Fun√ß√£o ass√≠ncrona para salvar hist√≥rico
def save_to_history_async(usuario, prompt, resposta):
    """Salva hist√≥rico de forma ass√≠ncrona para n√£o bloquear resposta"""
    def save_task():
        try:
            with get_db_connection() as conn:
                cursor = conn.cursor()
                cursor.execute(
                    'INSERT INTO historico (usuario, prompt, resposta) VALUES (?, ?, ?)',
                    (usuario, str(prompt), resposta)
                )
                conn.commit()
                print("‚úÖ Hist√≥rico salvo com sucesso")
        except Exception as e:
            print(f"‚ùå Erro ao salvar hist√≥rico: {e}")
    
    # Executar em thread separada
    thread = threading.Thread(target=save_task)
    thread.daemon = True
    thread.start()

@app.route('/api/chat', methods=['POST'])
def chat():
    start_time = time.time()
    
    try:
        data = request.json
        messages = data.get('messages', [])
        model = data.get('model', 'gpt-5')
        # OTIMIZA√á√ÉO: Reduzir max_tokens para evitar out of memory
        # GPT-5 funciona bem com 4000 tokens mantendo qualidade de an√°lise
        max_tokens = min(data.get('max_tokens', 4000), 32000)
        
        log_debug("üöÄ === NOVA REQUISI√á√ÉO DE AN√ÅLISE ===")
        log_debug(f"üìß Modelo: {model}")
        log_debug(f"üî¢ Max Tokens (otimizado): {max_tokens}")
        log_debug(f"üìù Total de mensagens: {len(messages)}")
        
        # üîç DEBUG: Mostrar conte√∫do das mensagens para diagnosticar problemas
        for idx, msg in enumerate(messages):
            role = msg.get('role', 'unknown')
            content_size = len(msg.get('content', ''))
            log_debug(f"   Mensagem {idx + 1} ({role}): {content_size} chars")
            if role == 'user' and content_size < 200:
                log_debug(f"      [‚ö†Ô∏è Mensagem do usu√°rio muito pequena!]")
                log_debug(f"      Conte√∫do: {repr(msg.get('content', '')[:100])}")
        
        log_debug("=" * 50)

        # Valida√ß√£o b√°sica
        if not messages:
            log_debug('‚ùå Nenhuma mensagem fornecida')
            return jsonify({'error': 'Nenhuma mensagem fornecida'}), 400
        
        # ‚ö†Ô∏è VALIDA√á√ÉO CR√çTICA: Se a mensagem do usu√°rio estiver vazia, √© um problema!
        user_message = next((m for m in messages if m.get('role') == 'user'), None)
        if not user_message or not user_message.get('content', '').strip():
            log_debug("‚ùå ERRO: Mensagem do usu√°rio est√° VAZIA!")
            log_debug(f"   Messages recebidas: {messages}")
            return jsonify({'error': 'Mensagem do usu√°rio est√° vazia - n√£o √© poss√≠vel processar'}), 400
        
        # OTIMIZA√á√ÉO: Reduzir limite de tamanho total para economizar mem√≥ria
        # Consolida√ß√£o muito grande de documentos causa out of memory
        tamanho_messages = len(str(messages))
        truncado = False
        
        if tamanho_messages > 30000:  # Reduzido de 50000 para 30000
            log_debug(f"‚ö†Ô∏è AVISO: Prompt muito longo ({tamanho_messages} chars), truncando documentos...")
            truncado = True
            # Truncar mensagem do usu√°rio se muito grande
            for msg in messages:
                if msg.get('role') == 'user' and len(msg.get('content', '')) > 20000:
                    tamanho_antes = len(msg.get('content', ''))
                    msg['content'] = msg['content'][:20000] + "\n\n[‚ö†Ô∏è DOCUMENTO TRUNCADO - LIMITE DE MEM√ìRIA DO SERVIDOR]"
                    log_debug(f"   üìÑ Documento reduzido de {tamanho_antes} para 20000 caracteres")
            log_debug("‚úÖ Documentos truncados com sucesso")

        # üîç LOG: Mostrar conte√∫do completo do prompt que ser√° enviado
        log_debug("üîç === PROMPT ENVIADO PARA OPENAI ===")
        for idx, msg in enumerate(messages):
            role = msg.get('role', 'unknown')
            content = msg.get('content', '')
            log_debug(f"\nüìå MENSAGEM {idx + 1} ({role}):")
            if role == 'system':
                log_debug(f"Primeiros 200 chars: {content[:200]}")
            else:
                log_debug(f"Primeiros 300 chars: {content[:300]}")
                log_debug(f"Total: {len(content)} caracteres")
        log_debug("=" * 50)
        
        # Processar requisi√ß√£o OpenAI
        response, error = process_openai_request(messages, model, max_tokens)
        
        if error:
            log_debug(f"‚ùå ERRO na API OpenAI: {error}")
            log_debug(f"   Mensagens que causaram erro: {len(messages)} mensagens")
            return jsonify({'error': f'Erro na API OpenAI: {error}'}), 500
        
        if not response or not response.choices:
            log_debug("‚ùå ERRO: Resposta vazia da OpenAI")
            return jsonify({'error': 'Resposta vazia da OpenAI'}), 500

        # ‚úÖ VALIDA√á√ÉO CR√çTICA: Verificar se content est√° vazio
        content = response.choices[0].message.content if response.choices[0].message else None
        
        # ‚úÖ VALIDA√á√ÉO RIGOROSA: Content n√£o pode ser None, vazio ou s√≥ espa√ßos
        if not content or not content.strip():
            log_debug("\n" + "="*60)
            log_debug("‚ùå ERRO CR√çTICO: Content vazio ou s√≥ espa√ßos!")
            log_debug(f"   Content recebido: {repr(content)}")
            log_debug(f"   Is None: {content is None}")
            log_debug(f"   Type: {type(content)}")
            log_debug(f"   Len: {len(content) if content else 0}")
            log_debug("="*60)
            
            # üîÑ RETRY: Tentar novamente
            retry_count = 0
            max_retries = 2
            content = None
            
            while retry_count < max_retries and (not content or not content.strip()):
                retry_count += 1
                log_debug(f"\nüîÑ üîÑ üîÑ ACIONANDO RETRY {retry_count} de {max_retries} üîÑ üîÑ üîÑ")
                
                # Aguardar um pouco antes de retry
                time.sleep(2)
                
                response2, error2 = process_openai_request(messages, model, max_tokens, retry_attempt=retry_count+1)
                if error2:
                    log_debug(f"‚ùå RETRY {retry_count} falhou com erro: {error2}")
                    continue
                
                log_debug(f"   ‚úÖ Retry {retry_count}: Resposta recebida")
                content = response2.choices[0].message.content if response2 and response2.choices else None
                log_debug(f"   Content retry {retry_count}: {repr(content[:100] if content else 'VAZIO')}")
            
            if not content or not content.strip():
                log_debug("‚ùå ERRO CR√çTICO: Todas as tentativas falharam!")
                log_debug(f"   Total de tentativas: {retry_count + 1}")
                log_debug("   ‚ö†Ô∏è GPT-5 continua retornando vazio")
                log_debug("   Poss√≠veis causas: Token limit atingido, API indispon√≠vel, ou conte√∫do muito longo")
                
                # Retornar mensagem mais informativa
                return jsonify({
                    'error': 'OpenAI n√£o conseguiu processar sua requisi√ß√£o. Poss√≠veis causas: documentos muito grandes, limite de tokens ou indisponibilidade da API. Tente novamente ou com documentos menores.'
                }), 500
            
            log_debug(f"‚úÖ ‚úÖ ‚úÖ RETRY BEM-SUCEDIDO! ‚úÖ ‚úÖ ‚úÖ")
            log_debug(f"   Tamanho do conte√∫do: {len(content)} chars")
            log_debug(f"   Primeiros 100 chars: {content[:100]}")
        
        processing_time = time.time() - start_time
        log_debug(f"‚úÖ Resposta da OpenAI recebida com sucesso!")
        log_debug(f"üìÑ Tamanho da resposta: {len(content)} caracteres")
        
        # VALIDA√á√ÉO: Avisar se a an√°lise pode estar incompleta
        if truncado and len(content) < 500:
            log_debug("‚ö†Ô∏è AVISO: Resposta muito curta - an√°lise pode estar incompleta!")
            log_debug(f"   Tamanho da resposta: {len(content)} caracteres")
            content += "\n\n‚ö†Ô∏è **AVISO:** A an√°lise pode estar incompleta devido ao tamanho dos documentos. Para an√°lise completa, envie documentos menores separadamente."
        
        log_debug(f"‚è±Ô∏è Tempo de processamento: {processing_time:.2f}s")
        log_debug("=" * 50)

        # Salvar hist√≥rico de forma ass√≠ncrona
        save_to_history_async(
            data.get('usuario', 'anonimo'),
            messages,
            content
        )

        return jsonify({
            'choices': [{
                'message': {
                    'content': content
                }
            }],
            'processing_time': round(processing_time, 2)
        })
        
    except Exception as e:
        processing_time = time.time() - start_time
        error_msg = f"Erro interno do servidor: {str(e)}"
        print(f"‚ùå ERRO GERAL: {error_msg}")
        print(f"‚è±Ô∏è Tempo at√© erro: {processing_time:.2f}s")
        print("=" * 50)
        return jsonify({'error': error_msg}), 500

@app.route('/api/health', methods=['GET'])
def health():
    """Endpoint de sa√∫de com informa√ß√µes detalhadas"""
    try:
        # Testar conex√£o com banco
        with get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute('SELECT COUNT(*) FROM historico')
            total_records = cursor.fetchone()[0]
        
        return jsonify({
            "status": "ok",
            "openai_configured": bool(os.getenv("OPENAI_API_KEY")),
            "database_working": True,
            "total_records": total_records,
            "timeout_config": {
                "request_timeout": REQUEST_TIMEOUT,
                "openai_timeout": OPENAI_TIMEOUT
            }
        })
    except Exception as e:
        return jsonify({
            "status": "error",
            "openai_configured": bool(os.getenv("OPENAI_API_KEY")),
            "database_working": False,
            "error": str(e)
        }), 500

@app.route('/api/historico', methods=['GET'])
def get_historico():
    """Endpoint otimizado para buscar hist√≥rico"""
    try:
        limit = request.args.get('limit', 50, type=int)
        offset = request.args.get('offset', 0, type=int)
        
        # Limitar resultados para evitar sobrecarga
        limit = min(limit, 100)
        
        with get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute(
                'SELECT id, usuario, prompt, resposta, data FROM historico ORDER BY data DESC LIMIT ? OFFSET ?',
                (limit, offset)
            )
            rows = cursor.fetchall()
            
            # Contar total de registros
            cursor.execute('SELECT COUNT(*) FROM historico')
            total = cursor.fetchone()[0]
        
        historico = [
            {
                'id': row[0],
                'usuario': row[1],
                'prompt': row[2][:500] + '...' if len(row[2]) > 500 else row[2],  # Truncar prompt longo
                'resposta': row[3][:1000] + '...' if len(row[3]) > 1000 else row[3],  # Truncar resposta longa
                'data': row[4]
            }
            for row in rows
        ]
        
        return jsonify({
            'historico': historico,
            'total': total,
            'limit': limit,
            'offset': offset
        })
    except Exception as e:
        print(f"‚ùå Erro ao buscar hist√≥rico: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/settings', methods=['GET'])
def get_settings():
    """Endpoint para recuperar configura√ß√µes do usu√°rio"""
    try:
        # Tentar obter API Key do header
        api_key = request.headers.get('X-API-Key') or request.args.get('api_key', 'default')
        
        with get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute(
                'SELECT modelo, max_tokens, chunk_size FROM configuracoes WHERE api_key = ?',
                (api_key,)
            )
            row = cursor.fetchone()
        
        if row:
            return jsonify({
                'modelo': row[0],
                'max_tokens': row[1],
                'chunk_size': row[2],
                'cached': False
            })
        else:
            # Retornar valores padr√£o se n√£o encontrado
            return jsonify({
                'modelo': 'gpt-5',
                'max_tokens': 8000,
                'chunk_size': 8000,
                'cached': True
            })
    except Exception as e:
        print(f"‚ùå Erro ao buscar configura√ß√µes: {e}")
        return jsonify({
            'modelo': 'gpt-5',
            'max_tokens': 8000,
            'chunk_size': 8000,
            'error': str(e),
            'cached': True
        }), 200  # Retornar 200 mesmo com erro para fallback

@app.route('/api/settings', methods=['POST'])
def save_settings():
    """Endpoint para salvar configura√ß√µes do usu√°rio"""
    try:
        data = request.json
        api_key = data.get('api_key', 'default')
        modelo = data.get('modelo', 'gpt-5')
        max_tokens = data.get('max_tokens', 8000)
        chunk_size = data.get('chunk_size', 8000)
        
        # Valida√ß√µes b√°sicas
        if max_tokens < 100 or max_tokens > 128000:
            return jsonify({'error': 'max_tokens deve estar entre 100 e 128000'}), 400
        
        if chunk_size < 100 or chunk_size > 128000:
            return jsonify({'error': 'chunk_size deve estar entre 100 e 128000'}), 400
        
        with get_db_connection() as conn:
            cursor = conn.cursor()
            # Tentar atualizar, sen√£o inserir (UPSERT)
            cursor.execute('''
                INSERT INTO configuracoes (api_key, modelo, max_tokens, chunk_size)
                VALUES (?, ?, ?, ?)
                ON CONFLICT(api_key) 
                DO UPDATE SET 
                    modelo = excluded.modelo,
                    max_tokens = excluded.max_tokens,
                    chunk_size = excluded.chunk_size,
                    data_atualizacao = CURRENT_TIMESTAMP
            ''', (api_key, modelo, max_tokens, chunk_size))
            conn.commit()
        
        print(f"‚úÖ Configura√ß√µes salvas para API Key: {api_key[:10]}...")
        return jsonify({
            'success': True,
            'message': 'Configura√ß√µes salvas com sucesso',
            'modelo': modelo,
            'max_tokens': max_tokens,
            'chunk_size': chunk_size
        })
    except Exception as e:
        print(f"‚ùå Erro ao salvar configura√ß√µes: {e}")
        return jsonify({'error': str(e)}), 500

# Rota otimizada para servir arquivos da pasta App-IA
@app.route('/App-IA/<path:filename>')
def serve_app_ia_files(filename):
    try:
        app_ia_path = os.path.join(os.path.dirname(os.path.dirname(__file__)))
        return send_from_directory(app_ia_path, filename, as_attachment=False, cache_timeout=3600)
    except Exception as e:
        print(f"‚ùå Erro ao servir arquivo App-IA {filename}: {e}")
        return jsonify({'error': 'Arquivo n√£o encontrado'}), 404

# üîç ENDPOINT DE DIAGN√ìSTICO - Testa GPT-5 diretamente
@app.route('/api/test-gpt5', methods=['POST'])
def test_gpt5():
    """
    Endpoint APENAS PARA DIAGN√ìSTICO - Testa GPT-5 com um prompt simples.
    √ötil para verificar se GPT-5 est√° retornando conte√∫do vazio.
    """
    start_time = time.time()
    
    try:
        data = request.get_json()
        test_message = data.get('message', 'Teste simples: responda "OK"')
        
        print("\n" + "="*60)
        print("üîç DIAGN√ìSTICO: Testando GPT-5 diretamente")
        print(f"   Mensagem de teste: {test_message[:50]}...")
        print("="*60)
        
        # Teste com mensagem simples
        messages = [
            {
                "role": "system",
                "content": "Voc√™ √© um assistente de diagn√≥stico. Responda breve e claramente."
            },
            {
                "role": "user", 
                "content": test_message
            }
        ]
        
        # Chamar process_openai_request com 4000 tokens (mesmo que /api/chat)
        response, error = process_openai_request(messages, 'gpt-5', 4000)
        
        if error:
            print(f"‚ùå ERRO: {error}")
            return jsonify({
                'status': 'erro',
                'erro': error,
                'tempo': round(time.time() - start_time, 2)
            }), 500
        
        if not response or not response.choices:
            print("‚ùå Resposta vazia (sem choices)")
            return jsonify({
                'status': 'erro',
                'erro': 'Resposta vazia (sem choices)',
                'tempo': round(time.time() - start_time, 2)
            }), 500
        
        content = response.choices[0].message.content
        
        print(f"\n‚úÖ DIAGN√ìSTICO COMPLETADO")
        print(f"   Content: {repr(content[:100] if content else 'VAZIO')}")
        print(f"   Tamanho: {len(content) if content else 0} chars")
        print(f"   Tempo: {time.time() - start_time:.2f}s")
        print("="*60 + "\n")
        
        return jsonify({
            'status': 'sucesso' if content else 'aviso',
            'content': content,
            'tamanho': len(content) if content else 0,
            'content_is_empty': not content or not content.strip(),
            'tempo': round(time.time() - start_time, 2)
        })
        
    except Exception as e:
        print(f"‚ùå ERRO em /api/test-gpt5: {e}")
        import traceback
        print(traceback.format_exc())
        return jsonify({
            'status': 'erro',
            'erro': str(e),
            'tempo': round(time.time() - start_time, 2)
        }), 500

# Rota otimizada para a p√°gina principal
@app.route('/')
def index():
    try:
        app_ia_path = os.path.dirname(os.path.dirname(__file__))
        return send_file(os.path.join(app_ia_path, 'index.html'))
    except Exception as e:
        print(f"‚ùå Erro ao servir p√°gina principal: {e}")
        return jsonify({'error': 'P√°gina n√£o encontrado'}), 404

# üîç ROTA DE DEBUG: Servir p√°gina de debug dos logs
@app.route('/debug-logs')
@app.route('/debug_logs.html')
def debug_logs_page():
    """Serve a p√°gina de debug dos logs"""
    try:
        app_ia_path = os.path.dirname(os.path.dirname(__file__))
        return send_file(os.path.join(app_ia_path, 'debug_logs.html'))
    except Exception as e:
        log_debug(f"‚ùå Erro ao servir debug_logs.html: {e}")
        return jsonify({'error': 'P√°gina de debug n√£o encontrada'}), 404

# üîç ROTA DE DEBUG: Acessar logs
@app.route('/api/debug-logs', methods=['GET'])
def get_debug_logs():
    """Retorna os √∫ltimos 100 logs do arquivo app_debug.log"""
    try:
        log_file = os.path.join(os.path.dirname(__file__), 'app_debug.log')
        if not os.path.exists(log_file):
            return jsonify({'logs': 'Nenhum log dispon√≠vel ainda'}), 200
        
        with open(log_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        # Retorna os √∫ltimos 200 logs
        recent_logs = ''.join(lines[-200:])
        
        return jsonify({
            'logs': recent_logs,
            'total_lines': len(lines),
            'last_updated': datetime.now().isoformat()
        }), 200
    except Exception as e:
        return jsonify({'error': str(e)}), 500

# üîç ROTA DE DEBUG: Limpar logs
@app.route('/api/clear-logs', methods=['POST'])
def clear_logs():
    """Limpa o arquivo de log"""
    try:
        log_file = os.path.join(os.path.dirname(__file__), 'app_debug.log')
        if os.path.exists(log_file):
            open(log_file, 'w').close()
            return jsonify({'status': 'Logs limpos com sucesso'}), 200
        return jsonify({'status': 'Arquivo de log n√£o existe'}), 200
    except Exception as e:
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    print("=" * 70)
    print("üöÄ TOOLS ENGENHARIA - DOCUMENT AI ANALYZER BACKEND")
    print("=" * 70)
    print("üåê Servidor Flask iniciado em: http://localhost:5000")
    print("ü§ñ OpenAI API: Configurada e pronta")
    print("üìä Endpoints dispon√≠veis:")
    print("   ‚Ä¢ POST /api/chat - An√°lise de documentos")
    print("   ‚Ä¢ GET  /api/health - Status do sistema")
    print("   ‚Ä¢ GET  /api/historico - Hist√≥rico de an√°lises")
    print("   ‚Ä¢ GET  / - Interface principal")
    print("=" * 70)
    print("üí° Logs da aplica√ß√£o aparecer√£o abaixo:")
    print("=" * 70)
    
    # Para desenvolvimento local
    app.run(debug=False, port=5000, threaded=True)

# Para produ√ß√£o (Render/Vercel)
app = app