import sqlite3
import os
import signal
import sys
import threading
import time
from contextlib import contextmanager

# Fun√ß√£o para inicializar o banco e criar tabela se n√£o existir
def init_db():
    try:
        db_path = os.path.join(os.path.dirname(__file__), 'historico_base.db')
        conn = sqlite3.connect(db_path, timeout=10)
        cursor = conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS historico (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                usuario TEXT,
                prompt TEXT,
                resposta TEXT,
                data DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        # Criar tabela de configura√ß√µes
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS configuracoes (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                api_key TEXT UNIQUE,
                modelo TEXT DEFAULT 'gpt-5',
                max_tokens INTEGER DEFAULT 8000,
                chunk_size INTEGER DEFAULT 8000,
                data_atualizacao DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        conn.commit()
        conn.close()
        print("‚úÖ Banco de dados inicializado com sucesso")
    except Exception as e:
        print(f"‚ùå Erro ao inicializar banco: {e}")

# Pool de conex√µes para SQLite
@contextmanager
def get_db_connection():
    conn = None
    try:
        db_path = os.path.join(os.path.dirname(__file__), 'historico_base.db')
        conn = sqlite3.connect(db_path, timeout=10)
        yield conn
    except Exception as e:
        if conn:
            conn.rollback()
        print(f"‚ùå Erro na conex√£o com banco: {e}")
        raise
    finally:
        if conn:
            conn.close()

# Inicializar banco ao iniciar app
init_db()

from flask import Flask, request, jsonify, send_from_directory, send_file
from flask_cors import CORS
import os
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()
app = Flask(__name__)
CORS(app)

# Configura√ß√µes de timeout
REQUEST_TIMEOUT = 120  # 2 minutos para requisi√ß√µes OpenAI
OPENAI_TIMEOUT = 90    # 1.5 minutos para OpenAI especificamente

# Middleware de monitoramento
@app.before_request
def before_request():
    request.start_time = time.time()

@app.after_request
def after_request(response):
    duration = time.time() - request.start_time
    if duration > 5:  # Log apenas requisi√ß√µes longas
        print(f"‚ö†Ô∏è Requisi√ß√£o lenta: {request.endpoint} - {duration:.2f}s")
    return response

# Middleware para limpeza de mem√≥ria
import gc

@app.teardown_appcontext  
def cleanup(exception):
    gc.collect()  # For√ßar garbage collection

# Tratamento de sinais para graceful shutdown
def signal_handler(signum, frame):
    print(f"\nüõë Recebido sinal {signum}. Finalizando aplica√ß√£o...")
    sys.exit(0)

signal.signal(signal.SIGTERM, signal_handler)
signal.signal(signal.SIGINT, signal_handler)

# Rota otimizada para servir arquivos est√°ticos 
@app.route('/static/<path:filename>')
def static_files(filename):
    try:
        static_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'analise-bid-ia-tools')
        return send_from_directory(static_path, filename, as_attachment=False, cache_timeout=3600)
    except Exception as e:
        print(f"‚ùå Erro ao servir arquivo est√°tico {filename}: {e}")
        return jsonify({'error': 'Arquivo n√£o encontrado'}), 404

# Inicializar o cliente OpenAI com configura√ß√µes otimizadas
client = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY"),
    timeout=OPENAI_TIMEOUT,
    max_retries=2
)

# Fun√ß√£o para processar requisi√ß√£o com timeout
def process_openai_request(messages, model, max_tokens):
    """Processa requisi√ß√£o OpenAI com controle de timeout"""
    try:
        # GPT-5 usa max_completion_tokens em vez de max_tokens
        # GPT-5 requer temperature=1 (n√£o suporta outros valores)
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            max_completion_tokens=max_tokens,  # Corrigido para GPT-5
            temperature=1,  # GPT-5 s√≥ aceita valor padr√£o (1)
            timeout=OPENAI_TIMEOUT
        )
        return response, None
    except Exception as e:
        return None, str(e)

# Fun√ß√£o ass√≠ncrona para salvar hist√≥rico
def save_to_history_async(usuario, prompt, resposta):
    """Salva hist√≥rico de forma ass√≠ncrona para n√£o bloquear resposta"""
    def save_task():
        try:
            with get_db_connection() as conn:
                cursor = conn.cursor()
                cursor.execute(
                    'INSERT INTO historico (usuario, prompt, resposta) VALUES (?, ?, ?)',
                    (usuario, str(prompt), resposta)
                )
                conn.commit()
                print("‚úÖ Hist√≥rico salvo com sucesso")
        except Exception as e:
            print(f"‚ùå Erro ao salvar hist√≥rico: {e}")
    
    # Executar em thread separada
    thread = threading.Thread(target=save_task)
    thread.daemon = True
    thread.start()

@app.route('/api/chat', methods=['POST'])
def chat():
    start_time = time.time()
    
    try:
        data = request.json
        messages = data.get('messages', [])
        model = data.get('model', 'gpt-5')
        # OTIMIZA√á√ÉO: Reduzir max_tokens para evitar out of memory
        # GPT-5 funciona bem com 4000 tokens mantendo qualidade de an√°lise
        max_tokens = min(data.get('max_tokens', 4000), 32000)
        
        print("üöÄ === NOVA REQUISI√á√ÉO DE AN√ÅLISE ===")
        print(f"üìß Modelo: {model}")
        print(f"üî¢ Max Tokens (otimizado): {max_tokens}")
        print(f"üìù Total de mensagens: {len(messages)}")
        
        # üîç DEBUG: Mostrar conte√∫do das mensagens para diagnosticar problemas
        for idx, msg in enumerate(messages):
            role = msg.get('role', 'unknown')
            content_size = len(msg.get('content', ''))
            print(f"   Mensagem {idx + 1} ({role}): {content_size} chars")
            if role == 'user' and content_size < 200:
                print(f"      [‚ö†Ô∏è Mensagem do usu√°rio muito pequena!]")
                print(f"      Conte√∫do: {repr(msg.get('content', '')[:100])}")
        
        print("=" * 50)

        # Valida√ß√£o b√°sica
        if not messages:
            return jsonify({'error': 'Nenhuma mensagem fornecida'}), 400
        
        # ‚ö†Ô∏è VALIDA√á√ÉO CR√çTICA: Se a mensagem do usu√°rio estiver vazia, √© um problema!
        user_message = next((m for m in messages if m.get('role') == 'user'), None)
        if not user_message or not user_message.get('content', '').strip():
            print("‚ùå ERRO: Mensagem do usu√°rio est√° VAZIA!")
            print(f"   Messages recebidas: {messages}")
            return jsonify({'error': 'Mensagem do usu√°rio est√° vazia - n√£o √© poss√≠vel processar'}), 400
        
        # OTIMIZA√á√ÉO: Reduzir limite de tamanho total para economizar mem√≥ria
        # Consolida√ß√£o muito grande de documentos causa out of memory
        tamanho_messages = len(str(messages))
        truncado = False
        
        if tamanho_messages > 30000:  # Reduzido de 50000 para 30000
            print(f"‚ö†Ô∏è AVISO: Prompt muito longo ({tamanho_messages} chars), truncando documentos...")
            truncado = True
            # Truncar mensagem do usu√°rio se muito grande
            for msg in messages:
                if msg.get('role') == 'user' and len(msg.get('content', '')) > 20000:
                    tamanho_antes = len(msg.get('content', ''))
                    msg['content'] = msg['content'][:20000] + "\n\n[‚ö†Ô∏è DOCUMENTO TRUNCADO - LIMITE DE MEM√ìRIA DO SERVIDOR]"
                    print(f"   üìÑ Documento reduzido de {tamanho_antes} para 20000 caracteres")
            print("‚úÖ Documentos truncados com sucesso")

        # üîç LOG: Mostrar conte√∫do completo do prompt que ser√° enviado
        print("üîç === PROMPT ENVIADO PARA OPENAI ===")
        for idx, msg in enumerate(messages):
            role = msg.get('role', 'unknown')
            content = msg.get('content', '')
            print(f"\nüìå MENSAGEM {idx + 1} ({role}):")
            if role == 'system':
                print(f"Primeiros 200 chars: {content[:200]}")
            else:
                print(f"Primeiros 300 chars: {content[:300]}")
                print(f"Total: {len(content)} caracteres")
        print("=" * 50)
        
        # Processar requisi√ß√£o OpenAI
        response, error = process_openai_request(messages, model, max_tokens)
        
        if error:
            print(f"‚ùå ERRO na API OpenAI: {error}")
            print(f"   Mensagens que causaram erro: {len(messages)} mensagens")
            return jsonify({'error': f'Erro na API OpenAI: {error}'}), 500
        
        if not response or not response.choices:
            print("‚ùå ERRO: Resposta vazia da OpenAI")
            return jsonify({'error': 'Resposta vazia da OpenAI'}), 500

        # ‚úÖ VALIDA√á√ÉO CR√çTICA: Verificar se content est√° vazio
        content = response.choices[0].message.content
        
        # ‚úÖ VALIDA√á√ÉO RIGOROSA: Content n√£o pode ser None, vazio ou s√≥ espa√ßos
        if not content or not content.strip():
            print("‚ùå ERRO CR√çTICO: Content vazio ou s√≥ espa√ßos!")
            print(f"   Content recebido: {repr(content)}")
            print(f"Response object: {response}")
            print(f"Response choices: {response.choices}")
            print(f"Message: {response.choices[0].message}")
            
            # üîÑ RETRY: Tentar novamente com temperature maior
            print("\nüîÑ Tentando novamente com par√¢metros ajustados...")
            response2, error2 = process_openai_request(messages, model, max_tokens)
            if error2:
                print(f"‚ùå RETRY falhou: {error2}")
                return jsonify({'error': 'OpenAI n√£o conseguiu gerar resposta - documentos podem estar corrompidos'}), 500
            
            content2 = response2.choices[0].message.content if response2 and response2.choices else ""
            if not content2 or not content2.strip():
                print("‚ùå ERRO CR√çTICO: Mesmo ap√≥s retry, resposta est√° vazia!")
                return jsonify({'error': 'OpenAI retornou resposta vazia mesmo ap√≥s tentativas - tente novamente'}), 500
            
            content = content2
            print(f"‚úÖ RETRY bem-sucedido! Tamanho: {len(content)} chars")
        
        processing_time = time.time() - start_time
        print(f"‚úÖ Resposta da OpenAI recebida com sucesso!")
        print(f"üìÑ Tamanho da resposta: {len(content)} caracteres")
        
        # VALIDA√á√ÉO: Avisar se a an√°lise pode estar incompleta
        if truncado and len(content) < 500:
            print("‚ö†Ô∏è AVISO: Resposta muito curta - an√°lise pode estar incompleta!")
            print(f"   Tamanho da resposta: {len(content)} caracteres")
            content += "\n\n‚ö†Ô∏è **AVISO:** A an√°lise pode estar incompleta devido ao tamanho dos documentos. Para an√°lise completa, envie documentos menores separadamente."
        
        print("‚úÖ Resposta da OpenAI recebida com sucesso!")
        print(f"üìÑ Tamanho da resposta: {len(content)} caracteres")
        print(f"‚è±Ô∏è Tempo de processamento: {processing_time:.2f}s")
        print("=" * 50)

        # Salvar hist√≥rico de forma ass√≠ncrona
        save_to_history_async(
            data.get('usuario', 'anonimo'),
            messages,
            content
        )

        return jsonify({
            'choices': [{
                'message': {
                    'content': content
                }
            }],
            'processing_time': round(processing_time, 2)
        })
        
    except Exception as e:
        processing_time = time.time() - start_time
        error_msg = f"Erro interno do servidor: {str(e)}"
        print(f"‚ùå ERRO GERAL: {error_msg}")
        print(f"‚è±Ô∏è Tempo at√© erro: {processing_time:.2f}s")
        print("=" * 50)
        return jsonify({'error': error_msg}), 500

@app.route('/api/health', methods=['GET'])
def health():
    """Endpoint de sa√∫de com informa√ß√µes detalhadas"""
    try:
        # Testar conex√£o com banco
        with get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute('SELECT COUNT(*) FROM historico')
            total_records = cursor.fetchone()[0]
        
        return jsonify({
            "status": "ok",
            "openai_configured": bool(os.getenv("OPENAI_API_KEY")),
            "database_working": True,
            "total_records": total_records,
            "timeout_config": {
                "request_timeout": REQUEST_TIMEOUT,
                "openai_timeout": OPENAI_TIMEOUT
            }
        })
    except Exception as e:
        return jsonify({
            "status": "error",
            "openai_configured": bool(os.getenv("OPENAI_API_KEY")),
            "database_working": False,
            "error": str(e)
        }), 500

@app.route('/api/historico', methods=['GET'])
def get_historico():
    """Endpoint otimizado para buscar hist√≥rico"""
    try:
        limit = request.args.get('limit', 50, type=int)
        offset = request.args.get('offset', 0, type=int)
        
        # Limitar resultados para evitar sobrecarga
        limit = min(limit, 100)
        
        with get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute(
                'SELECT id, usuario, prompt, resposta, data FROM historico ORDER BY data DESC LIMIT ? OFFSET ?',
                (limit, offset)
            )
            rows = cursor.fetchall()
            
            # Contar total de registros
            cursor.execute('SELECT COUNT(*) FROM historico')
            total = cursor.fetchone()[0]
        
        historico = [
            {
                'id': row[0],
                'usuario': row[1],
                'prompt': row[2][:500] + '...' if len(row[2]) > 500 else row[2],  # Truncar prompt longo
                'resposta': row[3][:1000] + '...' if len(row[3]) > 1000 else row[3],  # Truncar resposta longa
                'data': row[4]
            }
            for row in rows
        ]
        
        return jsonify({
            'historico': historico,
            'total': total,
            'limit': limit,
            'offset': offset
        })
    except Exception as e:
        print(f"‚ùå Erro ao buscar hist√≥rico: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/settings', methods=['GET'])
def get_settings():
    """Endpoint para recuperar configura√ß√µes do usu√°rio"""
    try:
        # Tentar obter API Key do header
        api_key = request.headers.get('X-API-Key') or request.args.get('api_key', 'default')
        
        with get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute(
                'SELECT modelo, max_tokens, chunk_size FROM configuracoes WHERE api_key = ?',
                (api_key,)
            )
            row = cursor.fetchone()
        
        if row:
            return jsonify({
                'modelo': row[0],
                'max_tokens': row[1],
                'chunk_size': row[2],
                'cached': False
            })
        else:
            # Retornar valores padr√£o se n√£o encontrado
            return jsonify({
                'modelo': 'gpt-5',
                'max_tokens': 8000,
                'chunk_size': 8000,
                'cached': True
            })
    except Exception as e:
        print(f"‚ùå Erro ao buscar configura√ß√µes: {e}")
        return jsonify({
            'modelo': 'gpt-5',
            'max_tokens': 8000,
            'chunk_size': 8000,
            'error': str(e),
            'cached': True
        }), 200  # Retornar 200 mesmo com erro para fallback

@app.route('/api/settings', methods=['POST'])
def save_settings():
    """Endpoint para salvar configura√ß√µes do usu√°rio"""
    try:
        data = request.json
        api_key = data.get('api_key', 'default')
        modelo = data.get('modelo', 'gpt-5')
        max_tokens = data.get('max_tokens', 8000)
        chunk_size = data.get('chunk_size', 8000)
        
        # Valida√ß√µes b√°sicas
        if max_tokens < 100 or max_tokens > 128000:
            return jsonify({'error': 'max_tokens deve estar entre 100 e 128000'}), 400
        
        if chunk_size < 100 or chunk_size > 128000:
            return jsonify({'error': 'chunk_size deve estar entre 100 e 128000'}), 400
        
        with get_db_connection() as conn:
            cursor = conn.cursor()
            # Tentar atualizar, sen√£o inserir (UPSERT)
            cursor.execute('''
                INSERT INTO configuracoes (api_key, modelo, max_tokens, chunk_size)
                VALUES (?, ?, ?, ?)
                ON CONFLICT(api_key) 
                DO UPDATE SET 
                    modelo = excluded.modelo,
                    max_tokens = excluded.max_tokens,
                    chunk_size = excluded.chunk_size,
                    data_atualizacao = CURRENT_TIMESTAMP
            ''', (api_key, modelo, max_tokens, chunk_size))
            conn.commit()
        
        print(f"‚úÖ Configura√ß√µes salvas para API Key: {api_key[:10]}...")
        return jsonify({
            'success': True,
            'message': 'Configura√ß√µes salvas com sucesso',
            'modelo': modelo,
            'max_tokens': max_tokens,
            'chunk_size': chunk_size
        })
    except Exception as e:
        print(f"‚ùå Erro ao salvar configura√ß√µes: {e}")
        return jsonify({'error': str(e)}), 500

# Rota otimizada para servir arquivos da pasta App-IA
@app.route('/App-IA/<path:filename>')
def serve_app_ia_files(filename):
    try:
        app_ia_path = os.path.join(os.path.dirname(os.path.dirname(__file__)))
        return send_from_directory(app_ia_path, filename, as_attachment=False, cache_timeout=3600)
    except Exception as e:
        print(f"‚ùå Erro ao servir arquivo App-IA {filename}: {e}")
        return jsonify({'error': 'Arquivo n√£o encontrado'}), 404

# Rota otimizada para a p√°gina principal
@app.route('/')
def index():
    try:
        app_ia_path = os.path.dirname(os.path.dirname(__file__))
        return send_file(os.path.join(app_ia_path, 'index.html'))
    except Exception as e:
        print(f"‚ùå Erro ao servir p√°gina principal: {e}")
        return jsonify({'error': 'P√°gina n√£o encontrada'}), 404

if __name__ == '__main__':
    print("=" * 70)
    print("üöÄ TOOLS ENGENHARIA - DOCUMENT AI ANALYZER BACKEND")
    print("=" * 70)
    print("üåê Servidor Flask iniciado em: http://localhost:5000")
    print("ü§ñ OpenAI API: Configurada e pronta")
    print("üìä Endpoints dispon√≠veis:")
    print("   ‚Ä¢ POST /api/chat - An√°lise de documentos")
    print("   ‚Ä¢ GET  /api/health - Status do sistema")
    print("   ‚Ä¢ GET  /api/historico - Hist√≥rico de an√°lises")
    print("   ‚Ä¢ GET  / - Interface principal")
    print("=" * 70)
    print("üí° Logs da aplica√ß√£o aparecer√£o abaixo:")
    print("=" * 70)
    
    # Para desenvolvimento local
    app.run(debug=False, port=5000, threaded=True)

# Para produ√ß√£o (Render/Vercel)
app = app